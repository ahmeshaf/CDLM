{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "CD-LM-qualitative.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BuIcGX2k9Vd2",
        "outputId": "51e5503f-658d-4544-f8fe-68b0c6471c73"
      },
      "source": [
        "!pip install transformers\n",
        "from google.colab import drive  # to mount Google Drive to Colab notebook\n",
        "drive.mount('/content/gdrive')\n",
        "path = './gdrive/My Drive/datasets/'\n",
        "import collections\n",
        "import pickle\n",
        "import numpy as np\n",
        "import json\n",
        "import pandas as pd\n",
        "from transformers import LongformerModel, LongformerTokenizer, LongformerForMaskedLM\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from typing import List, Tuple, Dict, Iterable, Any, Callable, Union\n",
        "import torch"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.9.2-py3-none-any.whl (2.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6 MB 4.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.0)\n",
            "Collecting huggingface-hub==0.0.12\n",
            "  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 43.1 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 38.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 47.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Installing collected packages: tokenizers, sacremoses, pyyaml, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.0.12 pyyaml-5.4.1 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.9.2\n",
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXU-hbLSKi3i"
      },
      "source": [
        "path_reg = './gdrive/My Drive/cd-mlm'\n",
        "# path_rand = './gdrive/My Drive/random_cdlm'\n",
        "# path_bb = './gdrive/My Drive/prefix_cdlm'\n",
        "path = 'allenai/longformer-base-4096'\n",
        "tokenizer = LongformerTokenizer.from_pretrained(path_reg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-gmJlIbS-wQ"
      },
      "source": [
        "\"\"\"\n",
        "Compute BERT’s attention matrices for each sequence\n",
        "\"\"\"\n",
        "def get_attention_for_sentence_aut(sentence: list, model: object, token_lst:list=None):\n",
        "  \"\"\" Returns a tensor of all BERT's attentions - shape (num_layers, num_heads, seq_length, seq_length) \"\"\"\n",
        "  inputs = torch.tensor(sentence, dtype=torch.long).unsqueeze(0)\n",
        "  input_ids = inputs\n",
        "  global_attention = torch.zeros_like(input_ids)\n",
        "  if token_lst is not None:\n",
        "    for tok in token_lst:\n",
        "      global_attention[:,tok] = 1\n",
        "  device = model.device\n",
        "  global_attention = global_attention.to(device)\n",
        "  input_ids = input_ids.to(device)\n",
        "  attentions = model(input_ids, global_attention_mask=global_attention)[-1] # attentions here is list of layers, \n",
        "  model.cpu()\n",
        "  attentions = torch.stack(attentions).squeeze()\n",
        "  if len(attentions.size()) == 4:\n",
        "    attentions = attentions.mean(-1)\n",
        "  attentions = attentions.mean((0,1)).detach().cpu().numpy()\n",
        "  return attentions \n",
        "\n",
        "def get_all_attention_maps_aut(sentences: List[int], token_lst:List[int], model:object) -> torch.Tensor:\n",
        "  return get_attention_for_sentence_aut(sentences[0], model, token_lst)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZCizhhbR4Ro"
      },
      "source": [
        "def check_word_relations_aut(first_w, second_w, what_doc, model, doc_to_look=2):\n",
        "  models = {'cdlm':'./gdrive/My Drive/cdlm', 'rand cdlm':'./gdrive/My Drive/random_cdlm', 'longformer':'allenai/longformer-base-4096'}\n",
        "  scores = {}\n",
        "  to_cut_idx = len(what_doc)\n",
        "  attn_data = get_all_attention_maps_aut([what_doc],first_w, model)\n",
        "  a = attn_data\n",
        "  cut_a = a[:to_cut_idx]\n",
        "  del a\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "  mean_of_rel = np.mean(cut_a[second_w])\n",
        "  cut_a[second_w[0]] = mean_of_rel\n",
        "  cut_a=np.delete(cut_a, second_w[1:] + first_w)\n",
        "\n",
        "  if len(second_w) > 1:\n",
        "    what_doc_trunc = np.delete(what_doc, second_w[1:])\n",
        "  else:\n",
        "    what_doc_trunc = what_doc\n",
        "  new_a = np.argsort(cut_a)\n",
        "  start = np.where(np.array(what_doc)==50266)[0][0]\n",
        "  forbidden = [0,2,50266,50265] + tokenizer.encode('.', add_special_tokens=False,  add_prefix_space=False)\\\n",
        "  + tokenizer.encode(',', add_special_tokens=False,  add_prefix_space=False) + tokenizer.encode(':', add_special_tokens=False,  add_prefix_space=False)\\\n",
        "  + tokenizer.encode('?', add_special_tokens=False,  add_prefix_space=False) + tokenizer.encode('!', add_special_tokens=False,  add_prefix_space=False)\n",
        "\n",
        "  if doc_to_look == 1:\n",
        "    new_a = np.array([x for i,x in enumerate(new_a) if x < start and what_doc_trunc[x] not in forbidden])\n",
        "  elif doc_to_look==2:\n",
        "    new_a = np.array([x for i,x in enumerate(new_a) if x > start+1 and what_doc_trunc[x] not in forbidden])\n",
        "  elif doc_to_look==3:\n",
        "    new_a = np.array([x for i,x in enumerate(new_a) if what_doc_trunc[x] not in forbidden])\n",
        "  cands_res = np.where(new_a==second_w[0])[0]\n",
        "  scores = cands_res/len(new_a)\n",
        "  return scores\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-W74wZSR6xh"
      },
      "source": [
        "from tqdm import tqdm\n",
        "with open('./gdrive/My Drive/datasets/ecb+/ecb_qual_2k_events.json') as json_file:\n",
        "  data = json.load(json_file)\n",
        "\n",
        "cdmlm_pos = []\n",
        "cdmlm_neg = []\n",
        "rand_pos = []\n",
        "rand_neg = []\n",
        "long_pos = []\n",
        "long_neg = []\n",
        "\n",
        "df = pd.DataFrame(columns=['CD-LM', 'Longformer', 'label'])\n",
        "models = {'CD-LM':'./gdrive/My Drive/cdlm', 'Longformer':'allenai/longformer-base-4096'}\n",
        "for m, path in models.items():\n",
        "  model = LongformerModel.from_pretrained(path, output_attentions=True)\n",
        "  model.resize_token_embeddings(len(tokenizer))\n",
        "  model.cuda()\n",
        "  for i,d in tqdm(enumerate(data)):\n",
        "    if len(d['toks']) > 2000:\n",
        "      continue\n",
        "    scores = check_word_relations_aut(d['start'],d['end'],d['toks'], model)\n",
        "    df.loc[i, m] = scores\n",
        "    if d['label'] == 1:\n",
        "      df.loc[i, 'label'] = 1\n",
        "    else:\n",
        "      df.loc[i, 'label'] = 0\n",
        "  df.to_csv('./gdrive/My Drive/datasets/ecb+/qual_results_2k.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AqF_A8QmB3DS",
        "outputId": "6766866b-0a0f-46d5-d69a-57824bbaf2a9"
      },
      "source": [
        "len(data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cCRmqzkO6Ka"
      },
      "source": [
        "poss = df[df['label']==1]\n",
        "negs = df[df['label']==0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1KSEgE2V6Pc",
        "outputId": "4af6ba3d-52c7-4717-f32a-54ee34d79cfe"
      },
      "source": [
        "poss.mean()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CD-LM         0.702665\n",
              "Rand CD-LM    0.691109\n",
              "Longformer    0.597223\n",
              "label         1.000000\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7BxTiIh0XiZM",
        "outputId": "f9533259-cac2-4b85-a40f-49493336eff7"
      },
      "source": [
        "negs.mean()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CD-LM         0.556482\n",
              "Rand CD-LM    0.544480\n",
              "Longformer    0.505287\n",
              "label         0.000000\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    }
  ]
}